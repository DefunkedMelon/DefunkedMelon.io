{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Piazza_logo.svg \"The piazza logo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Data Science Through Piazza Forum Data\n",
    "*A comprehensive guide to the data science pipeline by Elan Naideck and Erika Schlunk*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome aspiring data scientists! Have you ever wanted to do a \"machine learning\" or a \"big data?\" If so, this tutorial will show you how to get started with the data science pipeline, and will introduce you to industry standard tools for data analytics and visualization. This tutorial assumes you already know a thing or two about python, so if at any point you feel like you don't understand the code it may be wise to brush up on your python knowledge.\n",
    "\n",
    "Now for the actual project. In this tutorial we will be using data from our own data science class forums to analyze student and instructor participation. A professor might find like to know how well their teacher's assistants have been performing, or how much their students are participating in the discussions. On our journey of meta analysis we'll walk you through all 5 steps of the data lifecycle shown bellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Data_lifecycle.png \"The data lifecycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the many interconnecting arrows this is not a strictly linear process. As you go through this process yourself you will find out that the universe is conspiring to make our lives miserable and no step in this process is straight forward. At any point you may find yourself having to backtrack and wrangle more data, or collect more data, or wrangle more data. Or wrangle your data some more, and then wrangle it again for good measure. This tutorial can't show you exactly how to do it because the process it totally unique for every dataset you may find yourself working with. This tutorial will give you the tools and outline the process, but you'll have to use your brain to figure out how best to prepare and process your own data. Copying this code won't help you much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Environment And You"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get going on the real data science we have to talk about your programming environment. If you aren't familiar with the world of jupyter notebooks you're about be. If you are already familiar with jupyter notebooks, and have jupyter installed on your machine you can skip this part and move on to the required packages. These \"notebooks\" are industry standard for data science with python due to their publishability, modular design, and inline data visualization support. You can get basic instructions on how to install jupyter [here](https://jupyter.readthedocs.io/en/latest/install.html), but if you like spoilers and would like to see a much more comprehnsive guide on how to use jupyter you can go [here](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook). Don't worry about mastering notebooks. Just get it up and running so you can experiment with them and write your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python wasn't designed as a data science language, but through the magic of packages python can rival or even surpass other statistical languages like R (fight me stat majors). The beating heart of the python science ecosystem is the pandas package. Pandas is a data analysis library that has become ubiquitous in the data science world because of it's fast, well optimized data structures and intuitive design. It's the first thing you want to have installed when you begin any data science project, so if you don't already have it installed in your python envoronment do so now by using pip or conda, depending on your setup. More help for installing pandas can be found [here](https://pandas.pydata.org/pandas-docs/stable/install.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have pandas up and running you can import it using the usual python import statements. You'll be using it a lot so I strongly recommend importing it with an alias. Industry standard is to import it as pd. The full list of standard packages you want to install when starting any project are [pandas](https://pandas.pydata.org/), [numpy](https://numpy.org/), [matplotlib](https://matplotlib.org/), [scikit-learn](https://scikit-learn.org/stable/), and [seaborn](https://seaborn.pydata.org/). If you want more information on how to use each package and what they do just follow the links to their websites. For now we'll just import them and explain how to use them as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary data structure around which all of your work will around is the pandas dataframe. Dataframes store data like a giant excel spreadsheet, with named columns and indexed rows. If you already know a thing or two about pandas you can move on to data collection, otherwise [here](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) is a handy guide to all of the basic dataframe opperations that we will need as we proceed. You might want to keep this handy when writing your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pictured below is the average state of real world data. Reality is messy, and so is the data it produces. Data can be downloaded from government websites, retrieved by API calls, scrapped of of websites manuallty, computer generated, or beamed into your brain by hyperinteligent beings from another world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](dumpster-fire.gif \"The average state of real world data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our particular situation we're going to be pulling our data with an unnoficial API for the piazza forums that somebody was kind enough to post online. API stands for application programming interface, and it's one of the nicest ways you can get data. An API is a package or set of instructions released publicly but companies or other organizations that you can use to query data from their web servers. They send you the data, usually in some documented format that you can then parse into your pandas dataframes. Pandas also has functions for importing any csv files you may stumble across online if your data is in a file you can download. A good source of getting nice, clean data like that is [kaggle](https://www.kaggle.com/datasets) If collecting your data is more complicated than either of those methods may god, and [beautifulsoup4](https://pypi.org/project/beautifulsoup4/) help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully we have an API, so the first step is to install it. The Piazza API which can be found [here](https://pypi.org/project/piazza-api/), with full documentation [here](https://github.com/hfaran/piazza-api/) can be installed using the standard pip/conda methods, which saves us quite a bit of hastle. Once we have it installed in our python environment we can import it into our jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from piazza_api.rpc import PiazzaRPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see I didn't just import the api. Before I even wrote my import statement I read through the API documentation and figured out which part of the package I need for my particular application. For this project I'm going to need to be able to download class and post data. The PiazzaRPC class in the documentation seems to have the functions I need so I imported just that. When it comes to using API's *DOCUMENTATION IS YOUR FRIEND*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our API and we've read its documentation we can start collecting our data. The first step is to login to the API. Many APIs have access tied to online accounts, so you need to login with your credentials. Obviously I removed my own credentials from the code bellow before posting it online, but if you have a Piazza account you can fill in your own credentials and class id and the code should work just fine. Once you've logged in you should print out your data so you can perform a visual inspection to see how it's stuctured and if there is anything wrong with it. From there it's time to play data detective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fol': 'project3|',\n",
       " 'pin': 1,\n",
       " 'm': 1574114977824,\n",
       " 'rq': 0,\n",
       " 'id': 'k34o5uf2wb63ee',\n",
       " 'unique_views': 278,\n",
       " 'score': 278.0,\n",
       " 'is_new': False,\n",
       " 'version': 8,\n",
       " 'bucket_name': 'Pinned',\n",
       " 'bucket_order': 0,\n",
       " 'folders': ['project3'],\n",
       " 'nr': 423,\n",
       " 'main_version': 8,\n",
       " 'request_instructor': 0,\n",
       " 'log': [{'t': '2019-11-18T16:54:37Z', 'u': 'iv9hhjrk2iv2w7', 'n': 'create'},\n",
       "  {'t': '2019-11-18T16:54:59Z', 'u': 'jl3b2jmi38z3wh', 'n': 'followup'},\n",
       "  {'t': '2019-11-18T16:55:33Z', 'n': 'followup'},\n",
       "  {'t': '2019-11-18T16:56:31Z', 'u': 'iv9hhjrk2iv2w7', 'n': 'update'},\n",
       "  {'t': '2019-11-18T16:59:32Z', 'n': 'followup'},\n",
       "  {'t': '2019-11-18T17:00:49Z', 'n': 'followup'},\n",
       "  {'t': '2019-11-18T18:46:07Z', 'n': 'followup'},\n",
       "  {'t': '2019-11-18T22:09:37Z', 'u': 'iv9hhjrk2iv2w7', 'n': 'feedback'}],\n",
       " 'subject': 'Extension on Project 3',\n",
       " 'no_answer_followup': 0,\n",
       " 'num_favorites': 1,\n",
       " 'type': 'note',\n",
       " 'tags': ['instructor-note', 'pin', 'project3'],\n",
       " 'content_snipet': 'I&#39;m extending P3&#39;s due date to next Monday, 11/25.\\xa0 This project is a bit chaotic.\\n\\n(Please\\xa0make sure you have s',\n",
       " 'view_adjust': 0,\n",
       " 'modified': '2019-11-18T22:09:37Z',\n",
       " 'gd': 12,\n",
       " 'updated': '2019-11-18T16:54:37Z',\n",
       " 'status': 'active'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below is the class id. Needed for querying any data for the target class. Don't ask how I got it\n",
    "# class nid: jzlv5mrrqnn4tz\n",
    "nid = 'jzlv5mrrqnn4tz'\n",
    "\n",
    "# Create a piazza API instance and login to it\n",
    "# DON'T FORGET TO REMOVE THE PASSWORD BEFORE POSTING IT ONLINE\n",
    "p = PiazzaRPC()\n",
    "p.user_login(email='enaideck@umd.edu', password='chaseand1')\n",
    "\n",
    "# The returned data structure is a dictionary containing tons of data about the class\n",
    "# It has to be sifted through to get a post list\n",
    "class_data = p.get_my_feed(limit=2000, offset=0, sort=\"recent\", nid='jzlv5mrrqnn4tz')\n",
    "posts_dict = class_data['feed']\n",
    "posts_dict[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As predicted the data format the API is giving us is... difficult. How are we supposed to pull TA participation data from this? Who even are the TA's? To pull the information we want out from this tangled mess we're going to have to put out the dumpster fire. That's right it's time for..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as data wrangling, programmer's torture, or the universe's punishment for the inevitable ethical transgressions of your data science career. Whatever you call it, and whoever you pray too, data wrangling is an essential and massive part of the data science process. It's usually the most scary and time consuming step of any project. Data wrangling can look complicated, and very frightening so once you see your data for the first time and start to panic you need to close your eyes, take a deep breath, and stay focused on a specific goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](panic.png \"don't panic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate goal of the data wrangling process is to end up with a few nice beautiful pandas dataframes. Picture them in you mind, their golden radiance and easy data access flowing through you. If you plan ahead and stay focused data wrangling can become much more managable. With that being said, what does a good dataframe look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Data and You"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to planning out your dataframes there are a few easy rules to follow that will make sure your dataframe is easy to work with, and easy to understand if you end up giving your data to somebody else. If you follow these rules you can claim that your data is 'tidy'\n",
    "* Each variable must have its own column\n",
    "* Each observation must have its own row\n",
    "* Each value must have its own cell\n",
    "\n",
    "You will come across data that doesn't follow these rules way too often, and it's up to you to rid the world that filth one dataframe at a time. Tidy data examples and more information can be found [here](https://www.jeannicholashould.com/tidy-data-in-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good idea what our dataframe should look like we need to come up with a schema to organize our data into. There is no deterministic procedure to figuring this out. You have to consider the analysis you're trying to make with your data and the avaliable data to create a good schema. The goal of this example is to analyze the responsiveness of the class instructors and look for greater trends in post data. The challenge of getting the Piazza posts data into a neat dataframe is that the posts have a nested structure. A root post is made by a user, then followup sub-posts are made by students and instructors. Our dataframe is going to have to reflect the nested nature of this information. So the plan is to have each post or followup as each row in a dataframe. The varaibles are going to be:\n",
    "* date posted\n",
    "* poster (instructor name or just student)\n",
    "* type (note, question, followup, or feedback)\n",
    "* id (id of this post in case it needs to be accessed later)\n",
    "* root (post id of root post. Refers to itself if post is a root)\n",
    "* unique views\n",
    "* number of favorites\n",
    "* folder(s) posts can have more than one folder. This will be tricky to make work\n",
    "\n",
    "### Pre-Wrangling\n",
    "We want to know the folders the posts are in for our analysis later, but a single post can occupy more than one folder. Since putting lists inside a pandas dataframe in bad practice I'll one hot encode the folders instead. Each folder will have its own column of true or false values to represent wheather a post is in that folder. So before we start we need a list of the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quiz1',\n",
       " 'quiz2',\n",
       " 'quiz3',\n",
       " 'quiz4',\n",
       " 'quiz5',\n",
       " 'quiz6',\n",
       " 'quiz7',\n",
       " 'quiz8',\n",
       " 'quiz9',\n",
       " 'quiz10',\n",
       " 'quiz11',\n",
       " 'quiz12',\n",
       " 'project1',\n",
       " 'project2',\n",
       " 'project3',\n",
       " 'project4',\n",
       " 'midterm_exam',\n",
       " 'final_tutorial',\n",
       " 'other',\n",
       " 'project',\n",
       " 'exam',\n",
       " 'logistics']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = class_data['tags']['instructor']\n",
    "folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking under 'tags' and then 'instructor' for a list of folders may seem unintuitive. That's because it is. The only way to figure out how to parse your data out is to carefully scrutinize it visually. Next up we need a dictionary relating user ids to the instuctors and ta's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jzm53injueh49j': 'AP',\n",
       " 'iv9hhjrk2iv2w7': 'JD',\n",
       " 'jzshkyeh9xl2kq': 'NB',\n",
       " 'jzlv84yxs2i5yl': 'MD',\n",
       " 'jxkizzz9dmw5cl': 'VN',\n",
       " 'it1xxqm338f2wo': 'YZ',\n",
       " 'idx3g94pfq357h': 'CS',\n",
       " 'hcjqqmhat2i461': 'BB',\n",
       " 'jrgrxaenjy9t5': 'SD',\n",
       " 'i0var34o4c12tl': 'SH',\n",
       " 'jrhvjhc017y2ld': 'HW',\n",
       " 'iyd84opsi10ph': 'VSH',\n",
       " 'if99odwpfz81nq': 'AM',\n",
       " 'j6wht3l0itp543': 'WL'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staff = {}\n",
    "for i in range(len(posts_dict)):\n",
    "    for j in range(len(posts_dict[i]['log'])):\n",
    "        # We're not sure if the user tag is in the post log. We have to check. We also don't need duplicates.\n",
    "        if 'u' in posts_dict[i]['log'][j].keys() and not posts_dict[i]['log'][j]['u'] in staff.keys():\n",
    "            udata = p.get_users(user_ids=[posts_dict[i]['log'][j]['u']], nid=nid)[0]\n",
    "            if udata['role'] == 'ta' or udata['role'] == 'professor': # We're not interested in students\n",
    "                staff[udata['id']] = ''.join([x[0].upper() for x in udata['name'].split(' ')]) #\n",
    "                \n",
    "staff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see I've taken the class staff's names and converted them into initials to preserve their anonymitiy. It's easy to get careless as a data scientist and accidently leak huge amounts of user information. An ethical data scientist is a careful data scientist. We'd love to build our dataframe now but we imediatly run into another problem though. The post overview we collected in the first stage of the pipeline doesn't contain enough information about the child posts. We're going to have to use the API to collect more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/506\n",
      "10/506\n",
      "20/506\n",
      "30/506\n",
      "40/506\n",
      "50/506\n",
      "60/506\n",
      "70/506\n",
      "80/506\n",
      "90/506\n",
      "100/506\n",
      "110/506\n",
      "120/506\n",
      "130/506\n",
      "140/506\n",
      "150/506\n",
      "160/506\n",
      "170/506\n",
      "180/506\n",
      "190/506\n",
      "200/506\n",
      "210/506\n",
      "220/506\n",
      "230/506\n",
      "240/506\n",
      "250/506\n",
      "260/506\n",
      "270/506\n",
      "280/506\n",
      "290/506\n",
      "300/506\n",
      "310/506\n",
      "320/506\n",
      "330/506\n",
      "340/506\n",
      "350/506\n",
      "360/506\n",
      "370/506\n",
      "380/506\n",
      "390/506\n",
      "400/506\n",
      "410/506\n",
      "420/506\n",
      "430/506\n",
      "440/506\n",
      "450/506\n",
      "460/506\n",
      "470/506\n",
      "480/506\n",
      "490/506\n",
      "500/506\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# We have a unique problem we need to solve. To get follow up post information we have to go through the post dictionary\n",
    "# we created earlier and request the post with the API. The API limits how quickly we can do this so we have to run it in\n",
    "# batches. This is going to take forever.\n",
    "wait = 10\n",
    "batch = 10\n",
    "posts_list = []\n",
    "for i in range(0, len(posts_dict)):\n",
    "    posts_list += [p.content_get(cid=posts_dict[i]['id'], nid=nid)]\n",
    "    if (i % batch == 0): # Every batch wait for the designated time and print out how many posts we've processed\n",
    "        print(str(i) + \"/\" + str(len(posts_dict)))\n",
    "        time.sleep(wait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visually inspect the information to get an idea of what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['folders', 'nr', 'data', 'created', 'bucket_order', 'no_answer_followup', 'change_log', 'bucket_name', 'history', 'type', 'tags', 'tag_good', 'unique_views', 'children', 'tag_good_arr', 'id', 'config', 'status', 'request_instructor', 'request_instructor_me', 'bookmarked', 'num_favorites', 'my_favorite', 'is_bookmarked', 'is_tag_good', 'q_edits', 'i_edits', 's_edits', 't', 'default_anonymity'])\n",
      "dict_keys(['anon', 'folders', 'data', 'no_upvotes', 'subject', 'created', 'bucket_order', 'bucket_name', 'type', 'tag_good', 'children', 'tag_good_arr', 'no_answer', 'id', 'updated', 'config'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'anon': 'stud',\n",
       "  'data': 'k46bv5h2k9x27h',\n",
       "  'type': 'create',\n",
       "  'when': '2019-12-15T01:25:37Z'},\n",
       " {'anon': 'no',\n",
       "  'uid': 'iv9hhjrk2iv2w7',\n",
       "  'to': 'k46bv5gz60t27g',\n",
       "  'type': 'followup',\n",
       "  'when': '2019-12-15T04:00:12Z'}]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(posts_list[0].keys())\n",
    "print(posts_list[0]['children'][0].keys())\n",
    "posts_list[8]['change_log']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've downloaded the full posts list along with their children posts we have to cram them into a dataframe. This is made slightly more complicated by the tree-like structure of the posts. A post can have an arbitrary number of child posts, and each of those can have their own child posts to infititum. To handle this structure wer're going to need a recursive function to flatten the posts out so we can process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a root post, which has a tree-like structure and flattens it with its children into a list of posts\n",
    "def flatten_post(p, root = None, folders = None, views = None, favs = None):\n",
    "    # Since root posts contain more information than children posts we need to treat them differently\n",
    "    if root == None:\n",
    "        # Make a new dictionary that only stores the data we need\n",
    "        p_data = {'created':p['created'], 'folders':p['folders'], 'id':p['id'], 'type':p['type'], \n",
    "                  'views':p['unique_views'], 'num_favorites':p['num_favorites'], 'root':p['id']}\n",
    "        # Save the uid to the flattened post data\n",
    "        if 'uid' in p['change_log'][0].keys():\n",
    "            p_data['uid'] = p['change_log'][0]['uid']\n",
    "        else:\n",
    "            p_data['uid'] = 'anon'\n",
    "        # Recurse on the children\n",
    "        p_list = [p_data]\n",
    "        for child in p['children']:\n",
    "            p_list += flatten_post(child, root = p['id'], folders = p['folders'],\n",
    "                                   views = p['unique_views'], favs = p['num_favorites'])\n",
    "        return p_list\n",
    "    else:\n",
    "        p_data = {'created':p['created'], 'folders':folders, 'id':p['id'], 'type':p['type'],\n",
    "                  'views':views, 'num_favorites':favs, 'root':root}\n",
    "        # Save the uid to the flattened post data\n",
    "        if 'uid' in p.keys():\n",
    "            p_data['uid'] = p['uid']\n",
    "        else:\n",
    "            p_data['uid'] = 'anon'\n",
    "        # Recurse on the children\n",
    "        p_list = [p_data]\n",
    "        for child in p['children']:\n",
    "            p_list += flatten_post(child, root = root, folders = folders,\n",
    "                                   views = views, favs = favs)\n",
    "        return p_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oh yeah, it's all coming together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done prewrangling we can get down to the actual data wrangling. That beautiful dataframe is so close I can almost taste it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# We initialize the dataframe with a column for each variable we want to store\n",
    "posts = pd.DataFrame(columns = ['timestamp', 'poster', 'type', 'id', 'root', 'unique_views', 'num_favs'] + folders)\n",
    "\n",
    "# Now here's where all that hard work pays off\n",
    "i = 0 # We need to keep track of our dataframe index. j will be our root post list index\n",
    "for j in range(len(posts_list)):\n",
    "    new_posts = flatten_post(posts_list[j]) # Flatten each post\n",
    "    for post in new_posts: # Go through each post in our list and add it to the dataframe\n",
    "        posts.loc[i, 'timestamp'] = datetime.strptime(post['created'], '%Y-%m-%dT%H:%M:%SZ') # Must parse timestamps\n",
    "        if post['uid'] in staff.keys(): # Need to handle the cases when poster isn't a staffmember\n",
    "            posts.loc[i, 'poster'] = staff[post['uid']]\n",
    "        else:\n",
    "            posts.loc[i, 'poster'] = 'student'\n",
    "        posts.loc[i, 'type'] = post['type']\n",
    "        posts.loc[i, 'id'] = post['id']\n",
    "        posts.loc[i, 'root'] = post['root']\n",
    "        posts.loc[i, 'unique_views'] = post['views']\n",
    "        posts.loc[i, 'num_favs'] = post['num_favorites']\n",
    "        for folder in folders:\n",
    "            posts.loc[i, folder] = 0\n",
    "        for folder in post['folders']:\n",
    "            if folder in folders: posts.loc[i, folder] = 1\n",
    "        i += 1 # Move the dataframe pointer to the next empty slot\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>poster</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>root</th>\n",
       "      <th>unique_views</th>\n",
       "      <th>num_favs</th>\n",
       "      <th>quiz1</th>\n",
       "      <th>quiz2</th>\n",
       "      <th>quiz3</th>\n",
       "      <th>...</th>\n",
       "      <th>project1</th>\n",
       "      <th>project2</th>\n",
       "      <th>project3</th>\n",
       "      <th>project4</th>\n",
       "      <th>midterm_exam</th>\n",
       "      <th>final_tutorial</th>\n",
       "      <th>other</th>\n",
       "      <th>project</th>\n",
       "      <th>exam</th>\n",
       "      <th>logistics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-09 18:08:02</td>\n",
       "      <td>AP</td>\n",
       "      <td>poll</td>\n",
       "      <td>k3yr15r88u7667</td>\n",
       "      <td>k3yr15r88u7667</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-09 18:08:59</td>\n",
       "      <td>student</td>\n",
       "      <td>followup</td>\n",
       "      <td>k3yr2dr1iiy10r</td>\n",
       "      <td>k3yr15r88u7667</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-11 15:40:35</td>\n",
       "      <td>student</td>\n",
       "      <td>followup</td>\n",
       "      <td>k41gn8dijq91l8</td>\n",
       "      <td>k3yr15r88u7667</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-11 16:42:36</td>\n",
       "      <td>AP</td>\n",
       "      <td>feedback</td>\n",
       "      <td>k41iuzyny955xq</td>\n",
       "      <td>k3yr15r88u7667</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-12 20:53:43</td>\n",
       "      <td>student</td>\n",
       "      <td>followup</td>\n",
       "      <td>k4379sa041gq</td>\n",
       "      <td>k3yr15r88u7667</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp   poster      type              id            root  \\\n",
       "0  2019-12-09 18:08:02       AP      poll  k3yr15r88u7667  k3yr15r88u7667   \n",
       "1  2019-12-09 18:08:59  student  followup  k3yr2dr1iiy10r  k3yr15r88u7667   \n",
       "2  2019-12-11 15:40:35  student  followup  k41gn8dijq91l8  k3yr15r88u7667   \n",
       "3  2019-12-11 16:42:36       AP  feedback  k41iuzyny955xq  k3yr15r88u7667   \n",
       "4  2019-12-12 20:53:43  student  followup    k4379sa041gq  k3yr15r88u7667   \n",
       "\n",
       "  unique_views num_favs quiz1 quiz2 quiz3  ... project1 project2 project3  \\\n",
       "0          141        0     0     0     0  ...        0        0        0   \n",
       "1          141        0     0     0     0  ...        0        0        0   \n",
       "2          141        0     0     0     0  ...        0        0        0   \n",
       "3          141        0     0     0     0  ...        0        0        0   \n",
       "4          141        0     0     0     0  ...        0        0        0   \n",
       "\n",
       "  project4 midterm_exam final_tutorial other project exam logistics  \n",
       "0        0            0              1     0       0    0         0  \n",
       "1        0            0              1     0       0    0         0  \n",
       "2        0            0              1     0       0    0         0  \n",
       "3        0            0              1     0       0    0         0  \n",
       "4        0            0              1     0       0    0         0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! One beautiful dataframe ready for visualization and analysis. Just reading my code and instructions it might seem like this process is easy, or at least doesn't take very long. This took me over 6 hours. This forum data is particuarly nasty to wrangle, and figuring out how to parse the information I needed from an API I've never used before with cryptic and many times unhelpful field names was a grueling endevour. For every functioning block of code you see here there were a hundred I deleted. I had to write countless little test blocks to probe the data and see what information was hidden where. The process was made even more difficult by the fact that posts didn't always have the same information fields stored in them so it was easy to run into key errors. What I'm trying to say is that I wasn't joking when I said wrangling is the hardest, most time consuming part of the process. It's a lot of trial and error but pushing through is very rewarding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis, Hypothesis Testing and ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight and Policy Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erika was here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
